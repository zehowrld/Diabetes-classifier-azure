{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create a responsible AI dashboard to evaluate your models\n",
        "\n",
        "When you compare and evaluate your machine learning models, you'll want to review more than just their performance metric. Azure Machine Learning allows you to create responsible AI dashboard to explore how the model performs on different cohorts of the data. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the data\n",
        "\n",
        "To create the responsible AI dashboard, you'll need a training and test dataset, stored as Parquet files and registered as data assets.\n",
        "\n",
        "The data is currently stored as CSV files. Let's convert them to Parquet files. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677025462688
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# read training and test dataset\n",
        "df_training = pd.read_csv(\"train-data/diabetes.csv\")\n",
        "df_test = pd.read_csv(\"test-data/diabetes-test.csv\")\n",
        "\n",
        "# display the first few rows of the training dataset\n",
        "df_training.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677025466902
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "# convert data to table\n",
        "table_training = pa.Table.from_pandas(df_training)\n",
        "table_test = pa.Table.from_pandas(df_test)\n",
        "\n",
        "# write tables out to parquet\n",
        "pq.write_table(table_training, \"train-data/diabetes-training.parquet\", version=\"1.0\")\n",
        "pq.write_table(table_test, \"test-data/diabetes-test.parquet\", version=\"1.0\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Before you continue\n",
        "\n",
        "You'll need the latest version of the **azureml-ai-ml** package to run the code in this notebook. Run the cell below to verify that it is installed.\n",
        "\n",
        "> **Note**:\n",
        "> If the **azure-ai-ml** package is not installed, run `pip install azure-ai-ml` to install it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677025474944
        }
      },
      "outputs": [],
      "source": [
        "pip show azure-ai-ml"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect to your workspace\n",
        "\n",
        "With the required SDK packages installed, now you're ready to connect to your workspace.\n",
        "\n",
        "To connect to a workspace, we need identifier parameters - a subscription ID, resource group name, and workspace name. Since you're working with a compute instance, managed by Azure Machine Learning, you can use the default values to connect to the workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677025488380
        }
      },
      "outputs": [],
      "source": [
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677025518893
        }
      },
      "outputs": [],
      "source": [
        "# Get a handle to workspace\n",
        "ml_client = MLClient.from_config(credential=credential)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the data assets\n",
        "\n",
        "To create the responsible AI dashboard, you need to register the training and testing datasets as **MLtable** data assets. The MLtable data assets reference the Parquet files you created earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677025520971
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "train_data_path = \"train-data/\"\n",
        "test_data_path = \"test-data/\"\n",
        "data_version = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677025542446
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "input_train_data = \"diabetes_train_mltable\"\n",
        "input_test_data = \"diabetes_test_mltable\"\n",
        "\n",
        "try:\n",
        "    # Try getting data already registered in workspace\n",
        "    train_data = ml_client.data.get(\n",
        "        name=input_train_data,\n",
        "        version=data_version,\n",
        "    )\n",
        "    test_data = ml_client.data.get(\n",
        "        name=input_test_data,\n",
        "        version=data_version,\n",
        "    )\n",
        "except Exception as e:\n",
        "    train_data = Data(\n",
        "        path=train_data_path,\n",
        "        type=AssetTypes.MLTABLE,\n",
        "        description=\"RAI diabetes training data\",\n",
        "        name=input_train_data,\n",
        "        version=data_version,\n",
        "    )\n",
        "    ml_client.data.create_or_update(train_data)\n",
        "\n",
        "    test_data = Data(\n",
        "        path=test_data_path,\n",
        "        type=AssetTypes.MLTABLE,\n",
        "        description=\"RAI diabetes test data\",\n",
        "        name=input_test_data,\n",
        "        version=data_version,\n",
        "    )\n",
        "    ml_client.data.create_or_update(test_data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the pipeline to create the responsible AI dashboard\n",
        "\n",
        "To create the dashboard, you'll build a pipeline with prebuilt components that are registered by default in the Azure Machine Learning workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677025587559
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Get handle to azureml registry for the RAI built in components\n",
        "registry_name = \"azureml\"\n",
        "ml_client_registry = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=ml_client.subscription_id,\n",
        "    resource_group_name=ml_client.resource_group_name,\n",
        "    registry_name=registry_name,\n",
        ")\n",
        "print(ml_client_registry)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Register the model\n",
        "\n",
        "A machine learning model has already been trained for you. The model predicts whether a patient has diabetes. All model files are stored in the `model` folder. \n",
        "\n",
        "Register the model by pointing to the `model` folder and its contents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677025596451
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Model\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "file_model = Model(\n",
        "    path=\"model\",\n",
        "    type=AssetTypes.MLFLOW_MODEL,\n",
        "    name=\"local-mlflow-diabetes\",\n",
        "    description=\"Model created from local file.\",\n",
        ")\n",
        "model = ml_client.models.create_or_update(file_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the pipeline\n",
        "\n",
        "To create the responsible AI dashboard, you'll create a pipeline using the prebuilt components. You can choose which components to use, and which features you want to include in your dashboard. You'll create a dashboard that includes error analysis and model interpretability. \n",
        "\n",
        "Next to the responsible AI features, a pipeline to build a dashboard needs to include a component at the start to construct the dashboard, and a component at the end to gather all generated insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677025619196
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "model_name = model.name\n",
        "expected_model_id = f\"{model_name}:1\"\n",
        "azureml_model_id = f\"azureml:{expected_model_id}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677025626888
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "label = \"latest\"\n",
        "\n",
        "rai_constructor_component = ml_client_registry.components.get(\n",
        "    name=\"microsoft_azureml_rai_tabular_insight_constructor\", label=label\n",
        ")\n",
        "\n",
        "# we get latest version and use the same version for all components\n",
        "version = rai_constructor_component.version\n",
        "print(\"The current version of RAI built-in components is: \" + version)\n",
        "\n",
        "rai_erroranalysis_component = ml_client_registry.components.get(\n",
        "    name=\"microsoft_azureml_rai_tabular_erroranalysis\", version=version\n",
        ")\n",
        "\n",
        "rai_explanation_component = ml_client_registry.components.get(\n",
        "    name=\"microsoft_azureml_rai_tabular_explanation\", version=version\n",
        ")\n",
        "\n",
        "rai_gather_component = ml_client_registry.components.get(\n",
        "    name=\"microsoft_azureml_rai_tabular_insight_gather\", version=version\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When you've retrieved all components you want to use, you can build the pipeline and connect the components in the appropriate order:\n",
        "\n",
        "1. Construct the dashboard.\n",
        "1. Add error analysis.\n",
        "1. Add explanations.\n",
        "1. Gather all insights and visualize them in the dashboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677026156980
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import Input, dsl\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "compute_name = \"aml-cluster\"\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=compute_name,\n",
        "    description=\"RAI insights on diabetes data\",\n",
        "    experiment_name=f\"RAI_insights_{model_name}\",\n",
        ")\n",
        "def rai_decision_pipeline(\n",
        "    target_column_name, train_data, test_data\n",
        "):\n",
        "    # Initiate the RAIInsights\n",
        "    create_rai_job = rai_constructor_component(\n",
        "        title=\"RAI dashboard diabetes\",\n",
        "        task_type=\"classification\",\n",
        "        model_info=expected_model_id,\n",
        "        model_input=Input(type=AssetTypes.MLFLOW_MODEL, path=azureml_model_id),\n",
        "        train_dataset=train_data,\n",
        "        test_dataset=test_data,\n",
        "        target_column_name=target_column_name,\n",
        "    )\n",
        "    create_rai_job.set_limits(timeout=300)\n",
        "\n",
        "    # Add error analysis\n",
        "    error_job = rai_erroranalysis_component(\n",
        "        rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,\n",
        "    )\n",
        "    error_job.set_limits(timeout=300)\n",
        "\n",
        "    # Add explanations\n",
        "    explanation_job = rai_explanation_component(\n",
        "        rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,\n",
        "        comment=\"add explanation\", \n",
        "    )\n",
        "    explanation_job.set_limits(timeout=300)\n",
        "\n",
        "    # Combine everything\n",
        "    rai_gather_job = rai_gather_component(\n",
        "        constructor=create_rai_job.outputs.rai_insights_dashboard,\n",
        "        insight_3=error_job.outputs.error_analysis,\n",
        "        insight_4=explanation_job.outputs.explanation,\n",
        "    )\n",
        "    rai_gather_job.set_limits(timeout=300)\n",
        "\n",
        "    rai_gather_job.outputs.dashboard.mode = \"upload\"\n",
        "\n",
        "    return {\n",
        "        \"dashboard\": rai_gather_job.outputs.dashboard,\n",
        "    }"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the pipeline has been built, you need to define the two necessary inputs: the training and test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677025638275
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import Input\n",
        "target_feature = \"Diabetic\"\n",
        "\n",
        "diabetes_train_pq = Input(\n",
        "    type=\"mltable\",\n",
        "    path=f\"azureml:{input_train_data}:{data_version}\",\n",
        "    mode=\"download\",\n",
        ")\n",
        "diabetes_test_pq = Input(\n",
        "    type=\"mltable\",\n",
        "    path=f\"azureml:{input_test_data}:{data_version}\",\n",
        "    mode=\"download\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we'll put everything together: assign the inputs to the pipeline and set the target column (the predicted label)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677026333108
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "from azure.ai.ml import Output\n",
        "\n",
        "# Pipeline to construct the RAI Insights\n",
        "insights_pipeline_job = rai_decision_pipeline(\n",
        "    target_column_name=\"Diabetic\",\n",
        "    train_data=diabetes_train_pq,\n",
        "    test_data=diabetes_test_pq,\n",
        ")\n",
        "\n",
        "# Workaround to enable the download\n",
        "rand_path = str(uuid.uuid4())\n",
        "insights_pipeline_job.outputs.dashboard = Output(\n",
        "    path=f\"azureml://datastores/workspaceblobstore/paths/{rand_path}/dashboard/\",\n",
        "    mode=\"upload\",\n",
        "    type=\"uri_folder\",\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the pipeline\n",
        "\n",
        "When you've successfully built the pipeline, you can submit it. The following code will submit the pipeline and check the status of the pipeline. You can also view the pipeline's status in the studio. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677026340892
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import PipelineJob\n",
        "from IPython.core.display import HTML\n",
        "from IPython.display import display\n",
        "import time\n",
        "\n",
        "def submit_and_wait(ml_client, pipeline_job) -> PipelineJob:\n",
        "    created_job = ml_client.jobs.create_or_update(pipeline_job)\n",
        "    assert created_job is not None\n",
        "\n",
        "    print(\"Pipeline job can be accessed in the following URL:\")\n",
        "    display(HTML('<a href=\"{0}\">{0}</a>'.format(created_job.studio_url)))\n",
        "\n",
        "    while created_job.status not in [\n",
        "        \"Completed\",\n",
        "        \"Failed\",\n",
        "        \"Canceled\",\n",
        "        \"NotResponding\",\n",
        "    ]:\n",
        "        time.sleep(30)\n",
        "        created_job = ml_client.jobs.get(created_job.name)\n",
        "        print(\"Latest status : {0}\".format(created_job.status))\n",
        "    assert created_job.status == \"Completed\"\n",
        "    return created_job\n",
        "\n",
        "\n",
        "# This is the actual submission\n",
        "insights_job = submit_and_wait(ml_client, insights_pipeline_job)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When the pipeline is completed, you can review the dashboard in the studio. "
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "f2b2cd046deda8eabef1e765a11d0ec9aa9bd1d31d56ce79c815a38c323e14ec"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
